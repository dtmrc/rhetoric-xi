library(rvest)
library(readr)
library(dplyr)

# Set the base URL and the initial page number
base_url <- "https://interpret.csis.org/translations/?_paged="
page_number <- 1

# Set a flag to control the loop
more_pages <- TRUE

# Initialize an empty list to store the extracted information
results <- list()

# Loop until there are no more pages
while(more_pages) {
  
  # Construct the URL for the current page
  url <- paste0(base_url, page_number)
  
  # Read the HTML from the URL
  webpage <- read_html(url)
  
  # Extract the desired information from the webpage
  info <- html_nodes(webpage, ".info") %>% html_text()
  
  # Add the extracted information to the list
  results <- c(results, info)
  
  # Check if there are more pages
  more_pages <- !is.null(html_nodes(webpage, ".next"))
  
  # Increment the page number
  page_number <- page_number + 1
}

# Define a function to extract the text from a webpage
extract_elements <- function(url) {
  # Read the webpage into an R object
  webpage <- read_html(url)
  
  # Select the elements with the desired class
  elements <- html_nodes(webpage, ".translations__translation")
  
  # Extract the text from the webpage
  text <- html_text(elements)
  
  # Generate a unique filename for the text file
  filename <- paste0(substr(gsub("[^[:alnum:]]", "", url), 35, 65), ".txt")
  
  # Construct the path to the subfolder
  subfolder <- file.path("txt/interpret")
  
  # Combine the path and the filename
  filepath <- file.path(subfolder, filename)
  
  writeLines(text, filepath)
}

# Scrape the text from each URL
elements <- lapply(selected, extract_elements)