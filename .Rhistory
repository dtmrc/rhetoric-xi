install.packages("readtext")
devtools::install_github("kbenoit/quanteda.dictionaries")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("txt/", package = "readtext")
library(rtweet)
auth <- rtweet_app()
auth_sitrep()
library(rtweet)
auth <- rtweet_app()
tmls <- get_timelines(c("jake_p_harr"), n = 10)
tmls %>%
dplyr::filter(created_at > "2017-10-29") %>%
dplyr::group_by(screen_name) %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
library(rtweet)
library(dplyr)
library(ggplot2)
auth <- rtweet_app()
tmls <- get_timelines(c("jake_p_harr"), n = 10)
tmls %>%
dplyr::filter(created_at > "2017-10-29") %>%
dplyr::group_by(screen_name) %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
library(rtweet)
library(dplyr)
library(ggplot2)
auth <- rtweet_app()
tmls <- get_timelines(c("jake_p_harr"), n = 10)
tmls %>%
dplyr::filter(created_at > "2017-10-29") %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
library(rtweet)
library(dplyr)
library(ggplot2)
auth <- rtweet_app()
tmls <- get_timelines(c("nba"), n = 100)
tmls %>%
dplyr::filter(created_at > "2017-10-29") %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
library(rtweet)
library(dplyr)
library(ggplot2)
auth <- rtweet_app()
tmls <- get_timelines(c("nba"), n = 100)
tmls %>%
dplyr::filter(created_at > "2022-12-15") %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
tmls <- get_timelines(c("nba"), n = 100)
tmls
install.packages("tidyverse")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("nba"), n = 100)
tmsl
tmls
write.csv(tmls, file = "/Users/jake/burpy.csv")
tmls = data.frame(lapply(df_Place, as.character), stringsAsFactors=FALSE)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "/Users/jake/burpy.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("nba"), n = 100)
tmls
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "/Users/jake/Documents/Coding/tweeps/burpy.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("jake_p_harr"), n = 100)
tmls
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "/Users/jake/Documents/Coding/tweeps/burpy2.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 100)
tmls
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "/Users/jake/Documents/Coding/tweeps/burpy2.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 100)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "export/burpy.csv")
pwd
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 100)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 20, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
set.seed(10)
textplot_wordcloud(dfm_xi)
textplot_wordcloud(dfm_xi)
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 50, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 30, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 30, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 30, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
require(readtext)
require(quanteda)
library(rtweet)
library(ggplot2)
library(tidyverse)
library(quanteda.textplots)
# This creates a temporary directory, pulls the last 100 tweets from ChineseEmbinUS, and exports them as a CSV.
dir.create("tweeps/export")
auth <- rtweet_app()
tmls <- get_timeline(c("ChineseEmbinUS"), n = 100)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
# This runs the readtext command against the "full_text" field of the exported tweets CSV to ingest them into R and deletes the temporary folder and CSV file
rt_xi <- readtext("tweeps/export/*", text_field = "full_text")
unlink("tweeps/export", recursive = TRUE)
# This converts the readtext output into a corpus so that it can be analyzed using various quantdata packages
corpus_xi <- corpus(rt_xi)
# Now that the corpus is created in R, it can be converted to tokens for various types of analysis.
# The following examples include different ways of creating tokens from the corpus, including by words, sentences, and characters.
doc.tokens <- tokens(corpus_xi)
# doc.tokens.sentence <- tokens(corpus_xi, what = "sentence")
# doc.tokens.character <- tokens(corpus_xi, what = "character")
# This command removes punctuation, numbers and symbols from the existing doc.tokens.
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)
#This command removes English stopwords.
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
# This command shortens words to their stems to compare similar terms (e.g. Chin* for "China" and "Chinese")
#doc.tokens <- tokens_wordstem(doc.tokens)
#Now that the inputs are tokenized, we can create a document feature matrix for various types of analysis
doc.dfm.xi <- dfm(doc.tokens)
# Create a wordcloud from the DFM
# textplot_wordcloud(doc.dfm.xi)
# View 15 most recurring words
topfeatures(doc.dfm.xi, 15)
source("~/Documents/Coding/rhetoric-xi/txt/interpret-allnodes.R")
rlang::last_error()
source("~/Documents/Coding/rhetoric-xi/txt/interpret-allnodes.R")
source("~/Documents/Coding/rhetoric-xi/txt/interpret-allnodes.R")
source("~/Documents/Coding/rhetoric-xi/txt/interpret-allnodes.R")
xi_speeches <- readtext("txt/xi-speech/*.txt")
library(readtext)
xi_speeches <- readtext("txt/xi-speech/*.txt")
corpus_xi <- corpus(xi_speeches)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 30, verbose = FALSE)
doc.tokens <- tokens(corpus_xi)
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
doc.tokens <- tokens_wordstem(doc.tokens)
doc.dfm.xi <- dfm(doc.tokens)
textplot_wordcloud(doc.dfm.xi)
library(quanteda.textplots)
p
textplot_wordcloud(doc.dfm.xi)
# View 15 most recurring words
topfeatures(doc.dfm.xi, 15)
source("~/Documents/Coding/rhetoric-xi/txt/quanteda-test-new.R")
# View 15 most recurring words
topfeatures(doc.dfm.xi, 15)
# View 15 most recurring words
topfeatures(doc.dfm.xi, 50)
source("~/Documents/Coding/rhetoric-xi/txt/quanteda-test-new.R")
install.packages("sentimentr")
library(sentimentr)
sentiment(doc.dfm.xi)
head(doc.dfm.xi)
source("~/Documents/Coding/rhetoric-xi/txt/quanteda-test-new.R")
devtools::install_github("kbenoit/LIWCalike")
library(LIWCalike)
liwc(doc.dfm.xi)
liwcalike(doc.dfm.xi)
summary(corpus_xi)
kwic(corpus_xi, "contain")
kwic(doc.dfm.xi, "contain")
kwic(doc.dfm.xi, "United States")
kwic(corpus_xi, "United States")
kwic(corpus_xi, "competition")
devtools::install_github("kbenoit/quanteda.dictionaries")
source("~/Documents/Coding/rhetoric-xi/txt/xi-combine-speeches.R")
source("~/Documents/Coding/rhetoric-xi/txt/xi-combine-speeches.R")
source("~/Documents/Coding/rhetoric-xi/txt/xi-combine-speeches.R")
source("~/Documents/Coding/rhetoric-xi/txt/xi-combine-speeches.R")
source("~/Documents/Coding/rhetoric-xi/txt/xi-combine-speeches.R")
source("~/Documents/Coding/rhetoric-xi/txt/xi-combine-speeches.R")
source("~/Documents/Coding/rhetoric-xi/txt/combine-interpret.R")
source("~/Documents/Coding/rhetoric-xi/txt/combine-interpret.R")
source("~/Documents/Coding/rhetoric-xi/txt/combine-interpret.R")
source("~/Documents/Coding/rhetoric-xi/txt/combine-interpret.R")
dataframe <- read.csv("txt/xi-speech/*.csv")
getwd()
dataframe <- readtext(paste0(DATA_DIR, "txt/xi-speech/*.csv", text_field = "full_text"))
library(readtext)
dataframe <- readtext(paste0(DATA_DIR, "txt/xi-speech/*.csv", text_field = "full_text"))
dataframe <- readtext(paste0(DATA_DIR, "txt/xi-speech/", text_field = "full_text"))
dataframe <- readtext(paste0(DATA_DIR, "txt/xi-speech/*", text_field = "full_text"))
getwd()
dataframe <- readtext("txt/xi-speech/*.csv", text_field = "full_text")
head(dataframe)
doc.corpus <- corpus(dataframe)
summary(doc.corpus)
doc.tokens <- tokens(doc.corpus)
doc.dfm.final <- dfm(doc.tokens)
head(kwic(doc.dfm.final, "contain", window = 3))
head(kwic(doc.corpus, "contain", window = 3))
head(kwic(doc.corpus, "containment", window = 3))
head(kwic(doc.tokens, "containment", window = 3))
head(kwic(doc.tokens, "United States", window = 3))
head(kwic(doc.tokens, "United/States", window = 3))
head(kwic(doc.tokens, "United/ States", window = 3))
head(kwic(doc.tokens, "United", window = 3))
head(kwic(doc.tokens, "America", window = 3))
head(kwic(doc.tokens, "technology", window = 3))
head(kwic(doc.tokens, "prosper", window = 3))
topfeatures(doc.dfm.final, 25)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
doc.dfm.final <- dfm(doc.tokens)
topfeatures(doc.dfm.final, 25)
source("~/Documents/Coding/rhetoric-xi/txt/analyze-xis.R")
source("~/Documents/Coding/rhetoric-xi/txt/analyze-xis.R")
dataframe <- readtext("txt/xi-speech/*.csv", text_field = "full_text")
doc.corpus <- corpus(dataframe)
doc.tokens <- tokens(doc.corpus)
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE,
remove_numbers = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
doc.dfm.final <- dfm(doc.tokens)
topfeatures(doc.dfm.final, 25)
head(kwic(doc.tokens, "development", window = 3))
head(kwic(doc.tokens, "development"))
output_lsd <- liwcalike(doc.corpus, dictionary = data_dictionary_LaverGarry)
head(output_lsd)
head(output_lsd)
output_lsd <- liwcalike(doc.tokens, dictionary = data_dictionary_LaverGarry)
write_csv(output_lsd)
write_csv(output_lsd, "piece.csv")
containment <- head(kwic(doc.tokens, "contain"))
write_csv(containment, "kwic.csv")
dataframe <- readtext("txt/xi-speech/*.csv", text_field = "full_text")
doc.corpus <- corpus(dataframe)
doc.tokens <- tokens(doc.corpus)
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE,
remove_numbers = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
doc.tokens <- tokens_wordstem(doc.tokens)
doc.tokens <- tokens_tolower(doc.tokens)
doc.dfm.final <- dfm(doc.tokens)
topfeatures(doc.dfm.final, 25)
containment <- head(kwic(doc.tokens, "contain"))
containment <- head(kwic(doc.tokens, "contain"))
head(containment)
containment <- head(kwic(doc.tokens, "contai"))
containment <- head(kwic(doc.tokens, "contain"))
head(containment)
containment <- head(kwic(doc.tokens, "contai"))
head(containment)
containment <- head(kwic(doc.tokens, "containment"))
head(containment)
containment <- head(kwic(doc.tokens, "contain"))
head(containment)
containment <- head(kwic(doc.tokens, "competition"))
head(containment)
containment <- head(kwic(doc.tokens, "compet"))
head(containment)
containment <- head(kwic(doc.tokens, "china"))
head(containment)
containment <- kwic(doc.tokens, pattern = "compet*", valuetype = "regex", window = 3)
print(containment)
dataframe <- readtext("txt/xi-speech/*.csv", text_field = "full_text")
doc.corpus <- corpus(dataframe)
doc.tokens <- tokens(doc.corpus)
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE,
remove_numbers = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
doc.tokens <- tokens_tolower(doc.tokens)
doc.dfm.final <- dfm(doc.tokens)
topfeatures(doc.dfm.final, 25)
containment <- kwic(doc.tokens, pattern = "compet*", valuetype = "regex", window = 3)
print(containment)
as.phrase(tokens("United States"))
as.phrase(c("United States"))
as.phrase(doc.tokens("United States"))
as.phrase(tokens("United States"))
containment <- kwic(doc.tokens, pattern = "technol*", valuetype = "regex", window = 3)
print(containment)
containment <- kwic(doc.tokens, pattern = "democr*", valuetype = "regex", window = 3)
print(containment)
containment <- kwic(doc.tokens, pattern = "democr*", valuetype = "regex", window = 15)
containment <- kwic(doc.tokens, pattern = "democr*", valuetype = "regex", window = 15)
write_csv(containment, "democ.csv")
dataframe <- readtext("txt/xi-speech/*.csv", text_field = "full_text")
doc.corpus <- corpus(dataframe)
doc.tokens <- tokens(doc.corpus)
doc.tokens <- tokens_tolower(doc.tokens)
containment <- kwic(doc.tokens, pattern = "democr*", valuetype = "regex", window = 15)
write_csv(containment, "democ.csv")
