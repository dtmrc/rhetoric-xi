install.packages("readtext")
devtools::install_github("kbenoit/quanteda.dictionaries")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("txt/", package = "readtext")
library(rtweet)
auth <- rtweet_app()
auth_sitrep()
library(rtweet)
auth <- rtweet_app()
tmls <- get_timelines(c("jake_p_harr"), n = 10)
tmls %>%
dplyr::filter(created_at > "2017-10-29") %>%
dplyr::group_by(screen_name) %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
library(rtweet)
library(dplyr)
library(ggplot2)
auth <- rtweet_app()
tmls <- get_timelines(c("jake_p_harr"), n = 10)
tmls %>%
dplyr::filter(created_at > "2017-10-29") %>%
dplyr::group_by(screen_name) %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
library(rtweet)
library(dplyr)
library(ggplot2)
auth <- rtweet_app()
tmls <- get_timelines(c("jake_p_harr"), n = 10)
tmls %>%
dplyr::filter(created_at > "2017-10-29") %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
library(rtweet)
library(dplyr)
library(ggplot2)
auth <- rtweet_app()
tmls <- get_timelines(c("nba"), n = 100)
tmls %>%
dplyr::filter(created_at > "2017-10-29") %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
library(rtweet)
library(dplyr)
library(ggplot2)
auth <- rtweet_app()
tmls <- get_timelines(c("nba"), n = 100)
tmls %>%
dplyr::filter(created_at > "2022-12-15") %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
tmls <- get_timelines(c("nba"), n = 100)
tmls
install.packages("tidyverse")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("nba"), n = 100)
tmsl
tmls
write.csv(tmls, file = "/Users/jake/burpy.csv")
tmls = data.frame(lapply(df_Place, as.character), stringsAsFactors=FALSE)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "/Users/jake/burpy.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("nba"), n = 100)
tmls
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "/Users/jake/Documents/Coding/tweeps/burpy.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("jake_p_harr"), n = 100)
tmls
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "/Users/jake/Documents/Coding/tweeps/burpy2.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 100)
tmls
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "/Users/jake/Documents/Coding/tweeps/burpy2.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 100)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "export/burpy.csv")
pwd
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 100)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 20, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
set.seed(10)
textplot_wordcloud(dfm_xi)
textplot_wordcloud(dfm_xi)
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 50, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 30, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 30, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 30, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
require(readtext)
require(quanteda)
library(rtweet)
library(ggplot2)
library(tidyverse)
library(quanteda.textplots)
# This creates a temporary directory, pulls the last 100 tweets from ChineseEmbinUS, and exports them as a CSV.
dir.create("tweeps/export")
auth <- rtweet_app()
tmls <- get_timeline(c("ChineseEmbinUS"), n = 100)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
# This runs the readtext command against the "full_text" field of the exported tweets CSV to ingest them into R and deletes the temporary folder and CSV file
rt_xi <- readtext("tweeps/export/*", text_field = "full_text")
unlink("tweeps/export", recursive = TRUE)
# This converts the readtext output into a corpus so that it can be analyzed using various quantdata packages
corpus_xi <- corpus(rt_xi)
# Now that the corpus is created in R, it can be converted to tokens for various types of analysis.
# The following examples include different ways of creating tokens from the corpus, including by words, sentences, and characters.
doc.tokens <- tokens(corpus_xi)
# doc.tokens.sentence <- tokens(corpus_xi, what = "sentence")
# doc.tokens.character <- tokens(corpus_xi, what = "character")
# This command removes punctuation, numbers and symbols from the existing doc.tokens.
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)
#This command removes English stopwords.
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
# This command shortens words to their stems to compare similar terms (e.g. Chin* for "China" and "Chinese")
#doc.tokens <- tokens_wordstem(doc.tokens)
#Now that the inputs are tokenized, we can create a document feature matrix for various types of analysis
doc.dfm.xi <- dfm(doc.tokens)
# Create a wordcloud from the DFM
# textplot_wordcloud(doc.dfm.xi)
# View 15 most recurring words
topfeatures(doc.dfm.xi, 15)
