<<<<<<< HEAD
=======
install.packages("readtext")
devtools::install_github("kbenoit/quanteda.dictionaries")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("txt/", package = "readtext")
library(rtweet)
auth <- rtweet_app()
auth_sitrep()
library(rtweet)
auth <- rtweet_app()
tmls <- get_timelines(c("jake_p_harr"), n = 10)
tmls %>%
dplyr::filter(created_at > "2017-10-29") %>%
dplyr::group_by(screen_name) %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
library(rtweet)
library(dplyr)
library(ggplot2)
auth <- rtweet_app()
tmls <- get_timelines(c("jake_p_harr"), n = 10)
tmls %>%
dplyr::filter(created_at > "2017-10-29") %>%
dplyr::group_by(screen_name) %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
library(rtweet)
library(dplyr)
library(ggplot2)
auth <- rtweet_app()
tmls <- get_timelines(c("jake_p_harr"), n = 10)
tmls %>%
dplyr::filter(created_at > "2017-10-29") %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
library(rtweet)
library(dplyr)
library(ggplot2)
auth <- rtweet_app()
tmls <- get_timelines(c("nba"), n = 100)
tmls %>%
dplyr::filter(created_at > "2017-10-29") %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
library(rtweet)
library(dplyr)
library(ggplot2)
auth <- rtweet_app()
tmls <- get_timelines(c("nba"), n = 100)
tmls %>%
dplyr::filter(created_at > "2022-12-15") %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
tmls <- get_timelines(c("nba"), n = 100)
tmls
install.packages("tidyverse")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("nba"), n = 100)
tmsl
tmls
write.csv(tmls, file = "/Users/jake/burpy.csv")
tmls = data.frame(lapply(df_Place, as.character), stringsAsFactors=FALSE)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "/Users/jake/burpy.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("nba"), n = 100)
tmls
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "/Users/jake/Documents/Coding/tweeps/burpy.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("jake_p_harr"), n = 100)
tmls
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "/Users/jake/Documents/Coding/tweeps/burpy2.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 100)
tmls
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "/Users/jake/Documents/Coding/tweeps/burpy2.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 100)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "export/burpy.csv")
pwd
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 100)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 20, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
set.seed(10)
textplot_wordcloud(dfm_xi)
textplot_wordcloud(dfm_xi)
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 50, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 30, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
>>>>>>> c889b031367b0c2f1425c929e6251c92e0208026
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 30, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
require(readtext)
require(quanteda)
library(quanteda.textplots)
library(rtweet)
library(ggplot2)
library(tidyverse)
auth <- rtweet_app()
tmls <- get_timelines(c("ChineseEmbinUS"), n = 1000)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
## Loading required package: readtext
# get the data directory from readtext
DATA_DIR <- system.file("tweeps/export", package = "readtext")
rt_xi <- readtext(paste0(DATA_DIR, "tweeps/export/*"), text_field = "full_text")
corpus_xi <- corpus(rt_xi)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 30, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm_xi)
require(readtext)
require(quanteda)
library(rtweet)
library(ggplot2)
library(tidyverse)
library(quanteda.textplots)
# This creates a temporary directory, pulls the last 100 tweets from ChineseEmbinUS, and exports them as a CSV.
dir.create("tweeps/export")
auth <- rtweet_app()
tmls <- get_timeline(c("ChineseEmbinUS"), n = 100)
tmls = data.frame(lapply(tmls, as.character), stringsAsFactors=FALSE)
write.csv(tmls, file = "tweeps/export/burpy.csv")
# This runs the readtext command against the "full_text" field of the exported tweets CSV to ingest them into R and deletes the temporary folder and CSV file
rt_xi <- readtext("tweeps/export/*", text_field = "full_text")
unlink("tweeps/export", recursive = TRUE)
# This converts the readtext output into a corpus so that it can be analyzed using various quantdata packages
corpus_xi <- corpus(rt_xi)
# Now that the corpus is created in R, it can be converted to tokens for various types of analysis.
# The following examples include different ways of creating tokens from the corpus, including by words, sentences, and characters.
doc.tokens <- tokens(corpus_xi)
# doc.tokens.sentence <- tokens(corpus_xi, what = "sentence")
# doc.tokens.character <- tokens(corpus_xi, what = "character")
# This command removes punctuation, numbers and symbols from the existing doc.tokens.
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)
#This command removes English stopwords.
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
# This command shortens words to their stems to compare similar terms (e.g. Chin* for "China" and "Chinese")
#doc.tokens <- tokens_wordstem(doc.tokens)
#Now that the inputs are tokenized, we can create a document feature matrix for various types of analysis
doc.dfm.xi <- dfm(doc.tokens)
# Create a wordcloud from the DFM
# textplot_wordcloud(doc.dfm.xi)
# View 15 most recurring words
topfeatures(doc.dfm.xi, 15)
source("~/Documents/Coding/rhetoric-xi/txt/interpret-allnodes.R")
rlang::last_error()
source("~/Documents/Coding/rhetoric-xi/txt/interpret-allnodes.R")
source("~/Documents/Coding/rhetoric-xi/txt/interpret-allnodes.R")
source("~/Documents/Coding/rhetoric-xi/txt/interpret-allnodes.R")
xi_speeches <- readtext("txt/xi-speech/*.txt")
library(readtext)
xi_speeches <- readtext("txt/xi-speech/*.txt")
corpus_xi <- corpus(xi_speeches)
dfm_xi <- corpus_subset(corpus_xi) %>%
dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 30, verbose = FALSE)
doc.tokens <- tokens(corpus_xi)
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
doc.tokens <- tokens_wordstem(doc.tokens)
doc.dfm.xi <- dfm(doc.tokens)
textplot_wordcloud(doc.dfm.xi)
library(quanteda.textplots)
p
textplot_wordcloud(doc.dfm.xi)
# View 15 most recurring words
topfeatures(doc.dfm.xi, 15)
source("~/Documents/Coding/rhetoric-xi/txt/quanteda-test-new.R")
# View 15 most recurring words
topfeatures(doc.dfm.xi, 15)
# View 15 most recurring words
topfeatures(doc.dfm.xi, 50)
source("~/Documents/Coding/rhetoric-xi/txt/quanteda-test-new.R")
install.packages("sentimentr")
library(sentimentr)
sentiment(doc.dfm.xi)
head(doc.dfm.xi)
source("~/Documents/Coding/rhetoric-xi/txt/quanteda-test-new.R")
devtools::install_github("kbenoit/LIWCalike")
library(LIWCalike)
liwc(doc.dfm.xi)
liwcalike(doc.dfm.xi)
summary(corpus_xi)
kwic(corpus_xi, "contain")
kwic(doc.dfm.xi, "contain")
kwic(doc.dfm.xi, "United States")
kwic(corpus_xi, "United States")
kwic(corpus_xi, "competition")
<<<<<<< HEAD
install.packages("purrr")
source("~/Coding/rhetoric-xi/txt/scrape-combine-all.R")
source("~/Coding/rhetoric-xi/txt/scrape-combine-all.R")
source("~/Coding/rhetoric-xi/txt/UntitledR.R")
source("~/Coding/rhetoric-xi/txt/UntitledR.R")
source("~/Coding/rhetoric-xi/txt/UntitledR.R")
source("~/Coding/rhetoric-xi/txt/UntitledR.R")
source("~/Coding/rhetoric-xi/txt/UntitledR.R")
source("~/Coding/rhetoric-xi/txt/UntitledR.R")
source("~/Coding/rhetoric-xi/txt/UntitledR.R")
library(rvest)
library(readr)
library(dplyr)
testy_xi <- read_html("https://interpret.csis.org/translations/?_paged=1")
links <- html_nodes(testy_xi, "a")
hrefs <- html_attr(links, "href")
selected <- grep("https://interpret.csis.org/translations/", hrefs, value = TRUE)
# Define a function to extract the text from a webpage
selected <- grep("https://interpret.csis.org/translations/", hrefs, value = TRUE)
# Define a function to extract the text from a webpage
extract_elements <- function(url) {
# Read the webpage into an R object
webpage <- read_html(url)
node1 <- html_nodes(webpage, ".entry-header__title")
node2 <- html_nodes(webpage, ".post-meta__date")
node3 <- html_nodes(webpage, ".translations__translation")
node4 <- html_nodes(webpage, ".post-meta__name")
title <- html_text(node1)
date_pub <- html_text(node2)
date_pub <- gsub("Published ", "", date_pub)
full_text <- html_text(node3)
author <- unlist(lapply(node4, html_text))
author <- paste(author, collapse = ", ")
author <- iconv(author, "latin1", "ASCII", sub="")
df <- data.frame(title, author, date_pub, full_text)
# Generate a unique filename for the text file
filename <- title[1]
filename <- gsub("[^[:alnum:]]", "_", filename)
filename <- substr(filename, 1, 50)
# Construct the path to the subfolder
subfolder <- file.path("txt/interpret")
# Combine the path and the filename
filepath <- file.path(subfolder, paste0(filename, ".csv"))
write_csv(df, filepath)
}
# Scrape the text from each URL
elements <- lapply(selected, extract_elements)
source("~/Coding/rhetoric-xi/txt/UntitledR.R")
source("~/Coding/rhetoric-xi/txt/UntitledR.R")
library(rvest)
library(readr)
library(dplyr)
library(purrr)
pages <- 1:19
testy_xi <- map(pages, read_html, paste0("https://interpret.csis.org/translations/?_paged=", .))
source("~/Coding/rhetoric-xi/txt/UntitledR.R")
source("~/Coding/rhetoric-xi/txt/UntitledR.R")
source("~/Coding/rhetoric-xi/txt/UntitledR.R")
source("~/Coding/rhetoric-xi/txt/last-try-paginate.R")
source("~/Coding/rhetoric-xi/txt/last-try-paginate.R")
source("~/Coding/rhetoric-xi/txt/last-try-paginate.R")
source("~/Coding/rhetoric-xi/txt/best-csv-interpret-scraper.R")
source("~/Coding/rhetoric-xi/txt/best-csv-interpret-scraper.R")
source("~/Coding/rhetoric-xi/txt/best-csv-interpret-scraper.R")
source("~/Coding/rhetoric-xi/txt/best-csv-interpret-scraper.R")
source("~/Coding/rhetoric-xi/txt/best-csv-interpret-scraper.R")
source("~/Coding/rhetoric-xi/txt/best-csv-interpret-scraper.R")
source("~/Coding/rhetoric-xi/txt/best-csv-interpret-scraper.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
head(response)
writeLines(response_data, "response.csv")
source("~/Coding/rhetoric-xi/txt/best-csv-interpret-scraper.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
library(readr)
FARA_All_RegistrantDocs <- read_csv("fara/FARA_All_RegistrantDocs.csv")
View(FARA_All_RegistrantDocs)
read.csv("fara/FARA_All_RegistrantDocs.csv")
source("~/Coding/rhetoric-xi/txt/best-csv-interpret-scraper.R")
source("~/Coding/rhetoric-xi/fara/analyze-fara.R")
source("~/Coding/rhetoric-xi/fara/analyze-fara.R")
source("~/Coding/rhetoric-xi/fara/analyze-fara.R")
source("~/Coding/rhetoric-xi/fara/analyze-fara.R")
source("~/Coding/rhetoric-xi/fara/analyze-fara.R")
source("~/Coding/rhetoric-xi/fara/analyze-fara.R")
source("~/Coding/rhetoric-xi/fara/analyze-fara.R")
source("~/Coding/rhetoric-xi/fara/analyze-fara.R")
spec()
spec(url_list2)
# Read in the URL list from a CSV file
url_list <- read_csv("url_list2.csv")
spec()
# Read in the URL list from a CSV file
url_list <- read_csv("url_list2.csv")
download.file("https://efile.fara.gov/docs/6968-Registration-Statement-20210602-1.pdf")
download.file("https://efile.fara.gov/docs/6968-Registration-Statement-20210602-1.pdf", destfile = test.pdf, mode = "wb")
# Read in the URL list from a CSV file
url_list <- read.csv("url_list2.csv")
# Loop through each URL and download the PDF
for (i in 1:nrow(url_list)) {
url <- url_list[i, "url"]
filename <- url_list[i, "filename"]
download.file(url, destfile = filename, mode = "wb")
}
install.packages("pdftools")
# Load the necessary libraries
library(tidyverse)
library(pdftools)
# Read in the PDF file
pdf <- pdf_text("5884-Registration-Statement-20081006-1.pdf")
# Convert the PDF text to a data frame
df <- read_delim(pdf, delim = "|", col_names = c("field1", "field2", "field3"))
# Write the data frame to a CSV file
write_csv(df, "form_data.csv")
install.packages("tesseract")
# Read in the PDF file and convert it to a TIFF image
image <- pdf_convert("5884-Registration-Statement-20081006-1.pdf", format = "tiff")
# Extract the text from the image using OCR
text <- ocr(image)
library(tesseract)
library(tidyverse)
library(tesseract)
# Read in the PDF file and convert it to a TIFF image
image <- pdf_convert("5884-Registration-Statement-20081006-1.pdf", format = "tiff")
# Extract the text from the image using OCR
text <- ocr(image)
# Convert the OCR text to a data frame
df <- read_delim(text, delim = "|", col_names = c("field1", "field2", "field3"))
# Write the data frame to a CSV file
write_csv(df, "form_data.csv")
library(tidyverse)
library(tesseract)
# Read in the PDF file and convert it to a TIFF image
image <- pdf_convert("5884-Registration-Statement-20081006-1.pdf", format = "tiff")
# Extract the text from the image using OCR
text <- ocr(image)
# Convert the OCR text to a data frame
df <- read_delim(text, delim = "|", col_names = c("field1", "field2", "field3"))
# Write the data frame to a CSV file
write_csv(df, "form_data.csv")
source("~/Coding/rhetoric-xi/fara/analyze-fara.R")
# Load the necessary libraries
library(tesseract)
library(pdftools)
# Read in the PDF file and extract page 3
pdf <- pdf_convert("7194-Registration-Statement-20221123-1.pdf", pages = 3, format = "tiff")
# Extract the text from the image using OCR
text <- ocr(pdf)
# Save the TIFF image to a file
write_tiff(pdf, "page3.tiff")
library(pdftools)
# Get a list of the PDF files in the directory
file_list <- list.files(pattern = "*.pdf")
# Set the directory where you want to save the PDF files
output_dir <- "fara/registrations"
# Loop through each file and extract page 3
for (i in 1:length(file_list)) {
filename <- file_list[i]
pdf_page3 <- pdf_crop(filename, pages = 3)
pdf_save(pdf_page3, paste0(output_dir, "/", filename, "_page3.pdf"))
}
pdf_page3 <- pdf.crop(filename, pages = 3)
pdf_page3 <- pdfcrop(filename, pages = 3)
library(pdftools)
# Get a list of the PDF files in the directory
file_list <- list.files(pattern = "*.pdf")
# Set the directory where you want to save the PDF files
output_dir <- "fara/registrations"
# Loop through each file and extract page 3
for (i in 1:length(file_list)) {
filename <- file_list[i]
pdf_page3 <- pdf_crop(filename, pages = 3)
pdf_save(pdf_page3, paste0(output_dir, "/", filename, "_page3.pdf"))
}
library(pdftools)
# Get a list of the PDF files in the directory
file_list <- list.files(pattern = "*.pdf")
# Set the directory where you want to save the PDF files
output_dir <- "fara/registrations"
# Loop through each file and extract page 3
for (i in 1:length(file_list)) {
filename <- file_list[i]
pdf_page3 <- pdf_subset(filename, pages = 3)
pdf_save(pdf_page3, paste0(output_dir, "/", filename, "_page3.pdf"))
}
library(pdftools)
# Get a list of the PDF files in the directory
file_list <- list.files(pattern = "*.pdf")
# Set the directory where you want to save the PDF files
output_dir <- "fara/registrations"
# Loop through each file and extract page 3
for (i in 1:length(file_list)) {
filename <- file_list[i]
pdf_page3 <- pdf_subset(filename, pages = 3)
pdf_write(pdf_page3, paste0(output_dir, "/", filename, "_page3.pdf"))
}
library(pdftools)
# Get a list of the PDF files in the directory
file_list <- list.files(pattern = "*.pdf")
# Set the directory where you want to save the PDF files
output_dir <- "fara/registrations"
# Loop through each file and extract page 3
for (i in 1:length(file_list)) {
filename <- file_list[i]
pdf_page3 <- pdf_subset(filename, pages = 3)
pdf_render_page(pdf_page3, paste0(output_dir, "/", filename, "_page3.pdf"))
}
pdf_render_page(pdf_page3, paste0(output_dir, "/", filename, "_page3.pdf"))
library(pdftools)
# Get a list of the PDF files in the directory
file_list <- list.files(pattern = "*.pdf")
# Set the directory where you want to save the PDF files
output_dir <- "fara/registrations"
# Loop through each file and extract page 3
for (i in 1:length(file_list)) {
filename <- file_list[i]
pdf_page3 <- pdf_subset(filename, pages = 3)
}
library(pdftools)
# Get a list of the PDF files in the directory
file_list <- list.files(pattern = "*.pdf")
# Set the directory where you want to save the PDF files
output_dir <- "fara/registrations"
# Loop through each file and extract page 3
for (i in 1:length(file_list)) {
filename <- file_list[i]
pdf_page3 <- pdf_subset(filename, pages = 3)
}
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
cat(text)
source("~/Coding/rhetoric-xi/ocr-test.R")
pngfile <- pdf_convert('https://jeroen.github.io/images/ocrscan.pdf', dpi = 600)
pdftools::
pngfile <- pdftools::pdf_convert('https://jeroen.github.io/images/ocrscan.pdf', dpi = 600)
source("~/Coding/rhetoric-xi/ocr-test.R")
pngfile <- pdftools::pdf_convert("fara/registrations/6547-test.pdf", dpi = 600)
text <- tesseract::ocr(pngfile)
cat(text)
file <- "fara/registrations/6547-test.pdf"
test_fara <- pdf_text(file)
cat(test_fara)
setwd("fara/registrations"")
pwd
setwd("fara/registrations)
setwd("fara/registrations")
files <- list.files(pattern = "pdf$")
print(files)
opinions <-(files, pdf_text)
opinions <- (files, pdf_text)
opinions <- lapply(files, pdf_text)
length(opinions)
lapply(opinions, length)
install.packages("tm")
library(tm)
corp <- Corpus(URISource(files),
readerControl = list(reader = readPDF))
head(corp)
opinions.tdm <- TermDocumentMatrix(corp,
control =
list(removePunctuation = TRUE,
stopwords = TRUE,
tolower = TRUE,
stemming = TRUE,
removeNumbers = TRUE,
bounds = list(global = c(3, Inf))))
inspect(opinions.tdm[1:10,])
source("~/Coding/rhetoric-xi/fara/registrations/analyze-pdf.R")
install.packages("pdftools")
source("~/Coding/rhetoric-xi/fara/registrations/analyze-pdf.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/registrations/analyze-pdf.R")
source("~/Coding/rhetoric-xi/fara/registrations/analyze-pdf.R")
source("~/Coding/rhetoric-xi/fara/registrations/analyze-pdf.R")
# set the directory where the PDFs are stored
dir <- "fara/registrations"
# get a list of all the PDF files in the directory
pdf_files <- list.files(dir, pattern = "\\.pdf$")
# loop through each PDF file
for (pdf_file in pdf_files) {
# extract the text from the PDF file
pdf_text <- pdf_text(paste0(dir, pdf_file))
# search for the specific word within the PDF text
if (grep("china", pdf_text)) {
print(paste0(pdf_file, " contains the specific word"))
}
}
list.files()
# get a list of all the PDF files in the directory
pdf_files <- list.files(dir, pattern = ".pdf$")
# loop through each PDF file
for (pdf_file in pdf_files) {
# extract the text from the PDF file
pdf_text <- pdf_text(paste0(dir, pdf_file))
# search for the specific word within the PDF text
if (grep("China", pdf_text)) {
print(paste0(pdf_file, " contains the specific word"))
}
}
source("~/Coding/rhetoric-xi/fara/registrations/analyze-pdf.R")
source("~/Coding/rhetoric-xi/fara/registrations/analyze-pdf.R")
# get a list of all the PDF files in the directory
pdf_files <- list.files(dir, pattern = "pdf$")
cat(pdf_files)
# get a list of all the PDF files in the directory
pdf_files <- list.files(pattern = "pdf$")
cat(pdf_files)
source("~/Coding/rhetoric-xi/fara/registrations/analyze-pdf.R")
source("~/Coding/rhetoric-xi/fara/registrations/analyze-pdf.R")
cat(pdf_files)
# extract the text from the PDF file
pdf_text <- pdf_text(paste0("fara/registrations/", pdf_file))
source("~/Coding/rhetoric-xi/fara/registrations/analyze-pdf.R")
source("~/Coding/rhetoric-xi/fara/registrations/analyze-pdf.R")
source("~/Coding/rhetoric-xi/fara/registrations/analyze-pdf.R")
# get a list of all the PDF files in the directory
pdf_files <- list.files(dir, pattern = "\\.pdf$")
# loop through each PDF file
for (pdf_file in pdf_files) {
# get information about the PDF file
pdf_info <- pdf_info(paste0(dir, pdf_file))
# print the information
print(pdf_info)
}
# get a list of all the PDF files in the directory
pdf_files <- list.files(dir, pattern = "pdf$")
# loop through each PDF file
for (pdf_file in pdf_files) {
# get information about the PDF file
pdf_info <- pdf_info(paste0(dir, pdf_file))
# print the information
print(pdf_info)
}
# get a list of all the PDF files in the directory
pdf_files <- list.files(pattern = "pdf$")
# loop through each PDF file
for (pdf_file in pdf_files) {
# get information about the PDF file
pdf_info <- pdf_info(paste0(dir, pdf_file))
# print the information
print(pdf_info)
}
source("~/Coding/rhetoric-xi/fara/analyze-fara.R")
csv_file <- "fara/registrations/numbers.csv"
# Initialize an empty data frame to store the results
results <- data.frame()
csv_file <- "fara/registrations/numbers.csv"
source("~/Coding/rhetoric-xi/fara/get-ch-fara.R")
csv_file <- read.csv("fara/registrations/numbers.csv")
csv_file <- read.csv("numbers.csv")
source("~/Coding/rhetoric-xi/fara/get-ch-fara.R")
source("~/Coding/rhetoric-xi/fara/get-ch-fara.R")
source("~/Coding/rhetoric-xi/fara/get-ch-fara.R")
source("~/Coding/rhetoric-xi/fara/get-ch-fara.R")
source("~/Coding/rhetoric-xi/fara/get-ch-fara.R")
install.packages("plyr")
source("~/Coding/rhetoric-xi/fara/get-ch-fara.R")
source("~/Coding/rhetoric-xi/fara/get-ch-fara.R")
source("~/Coding/rhetoric-xi/fara/get-ch-fara.R")
source("~/Coding/rhetoric-xi/fara/get-ch-fara.R")
source("~/Coding/rhetoric-xi/fara/get-ch-fara.R")
source("~/Coding/rhetoric-xi/fara/get-ch-fara.R")
pwd
setwd("")
setwd("fara")
getwd
getwd()
setwd("")
setwd("/")
getwd()
setwd("rhetoric-xi")
getwd()
setwd("fara")
setwd("fara/registrations")
getwd()
=======
devtools::install_github("kbenoit/quanteda.dictionaries")
source("~/Documents/Coding/rhetoric-xi/txt/xi-combine-speeches.R")
source("~/Documents/Coding/rhetoric-xi/txt/xi-combine-speeches.R")
source("~/Documents/Coding/rhetoric-xi/txt/xi-combine-speeches.R")
source("~/Documents/Coding/rhetoric-xi/txt/xi-combine-speeches.R")
source("~/Documents/Coding/rhetoric-xi/txt/xi-combine-speeches.R")
source("~/Documents/Coding/rhetoric-xi/txt/xi-combine-speeches.R")
source("~/Documents/Coding/rhetoric-xi/txt/combine-interpret.R")
source("~/Documents/Coding/rhetoric-xi/txt/combine-interpret.R")
source("~/Documents/Coding/rhetoric-xi/txt/combine-interpret.R")
source("~/Documents/Coding/rhetoric-xi/txt/combine-interpret.R")
dataframe <- read.csv("txt/xi-speech/*.csv")
getwd()
dataframe <- readtext(paste0(DATA_DIR, "txt/xi-speech/*.csv", text_field = "full_text"))
library(readtext)
dataframe <- readtext(paste0(DATA_DIR, "txt/xi-speech/*.csv", text_field = "full_text"))
dataframe <- readtext(paste0(DATA_DIR, "txt/xi-speech/", text_field = "full_text"))
dataframe <- readtext(paste0(DATA_DIR, "txt/xi-speech/*", text_field = "full_text"))
getwd()
dataframe <- readtext("txt/xi-speech/*.csv", text_field = "full_text")
head(dataframe)
doc.corpus <- corpus(dataframe)
summary(doc.corpus)
doc.tokens <- tokens(doc.corpus)
doc.dfm.final <- dfm(doc.tokens)
head(kwic(doc.dfm.final, "contain", window = 3))
head(kwic(doc.corpus, "contain", window = 3))
head(kwic(doc.corpus, "containment", window = 3))
head(kwic(doc.tokens, "containment", window = 3))
head(kwic(doc.tokens, "United States", window = 3))
head(kwic(doc.tokens, "United/States", window = 3))
head(kwic(doc.tokens, "United/ States", window = 3))
head(kwic(doc.tokens, "United", window = 3))
head(kwic(doc.tokens, "America", window = 3))
head(kwic(doc.tokens, "technology", window = 3))
head(kwic(doc.tokens, "prosper", window = 3))
topfeatures(doc.dfm.final, 25)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
doc.dfm.final <- dfm(doc.tokens)
topfeatures(doc.dfm.final, 25)
source("~/Documents/Coding/rhetoric-xi/txt/analyze-xis.R")
source("~/Documents/Coding/rhetoric-xi/txt/analyze-xis.R")
dataframe <- readtext("txt/xi-speech/*.csv", text_field = "full_text")
doc.corpus <- corpus(dataframe)
doc.tokens <- tokens(doc.corpus)
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE,
remove_numbers = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
doc.dfm.final <- dfm(doc.tokens)
topfeatures(doc.dfm.final, 25)
head(kwic(doc.tokens, "development", window = 3))
head(kwic(doc.tokens, "development"))
output_lsd <- liwcalike(doc.corpus, dictionary = data_dictionary_LaverGarry)
head(output_lsd)
head(output_lsd)
output_lsd <- liwcalike(doc.tokens, dictionary = data_dictionary_LaverGarry)
write_csv(output_lsd)
write_csv(output_lsd, "piece.csv")
containment <- head(kwic(doc.tokens, "contain"))
write_csv(containment, "kwic.csv")
dataframe <- readtext("txt/xi-speech/*.csv", text_field = "full_text")
doc.corpus <- corpus(dataframe)
doc.tokens <- tokens(doc.corpus)
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE,
remove_numbers = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
doc.tokens <- tokens_wordstem(doc.tokens)
doc.tokens <- tokens_tolower(doc.tokens)
doc.dfm.final <- dfm(doc.tokens)
topfeatures(doc.dfm.final, 25)
containment <- head(kwic(doc.tokens, "contain"))
containment <- head(kwic(doc.tokens, "contain"))
head(containment)
containment <- head(kwic(doc.tokens, "contai"))
containment <- head(kwic(doc.tokens, "contain"))
head(containment)
containment <- head(kwic(doc.tokens, "contai"))
head(containment)
containment <- head(kwic(doc.tokens, "containment"))
head(containment)
containment <- head(kwic(doc.tokens, "contain"))
head(containment)
containment <- head(kwic(doc.tokens, "competition"))
head(containment)
containment <- head(kwic(doc.tokens, "compet"))
head(containment)
containment <- head(kwic(doc.tokens, "china"))
head(containment)
containment <- kwic(doc.tokens, pattern = "compet*", valuetype = "regex", window = 3)
print(containment)
dataframe <- readtext("txt/xi-speech/*.csv", text_field = "full_text")
doc.corpus <- corpus(dataframe)
doc.tokens <- tokens(doc.corpus)
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE,
remove_numbers = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
doc.tokens <- tokens_tolower(doc.tokens)
doc.dfm.final <- dfm(doc.tokens)
topfeatures(doc.dfm.final, 25)
containment <- kwic(doc.tokens, pattern = "compet*", valuetype = "regex", window = 3)
print(containment)
as.phrase(tokens("United States"))
as.phrase(c("United States"))
as.phrase(doc.tokens("United States"))
as.phrase(tokens("United States"))
containment <- kwic(doc.tokens, pattern = "technol*", valuetype = "regex", window = 3)
print(containment)
containment <- kwic(doc.tokens, pattern = "democr*", valuetype = "regex", window = 3)
print(containment)
containment <- kwic(doc.tokens, pattern = "democr*", valuetype = "regex", window = 15)
containment <- kwic(doc.tokens, pattern = "democr*", valuetype = "regex", window = 15)
write_csv(containment, "democ.csv")
dataframe <- readtext("txt/xi-speech/*.csv", text_field = "full_text")
doc.corpus <- corpus(dataframe)
doc.tokens <- tokens(doc.corpus)
doc.tokens <- tokens_tolower(doc.tokens)
containment <- kwic(doc.tokens, pattern = "democr*", valuetype = "regex", window = 15)
write_csv(containment, "democ.csv")
>>>>>>> c889b031367b0c2f1425c929e6251c92e0208026
